import os
import numpy as np
from skimage.io import imread, imshow
from skimage.transform import resize
import matplotlib.pyplot as plt
from tqdm import tqdm
import zipfile
import random
from google.colab import drive
import tensorflow as tf


# Habilitar google drive
drive.mount('/content/drive')


# Directorio del dataset
TEST_PATH = '/content/drive/MyDrive/TESIS/Datos3/val/'

# Imágenes y máscaras de entrenamiento -> Listado y ordenado
test_images_ids = sorted(os.listdir(TEST_PATH + 'images/'))
test_masks_ids = sorted(os.listdir(TEST_PATH + 'masks/'))

# Arreglos vacíos para imágenes y máscaras
X_test = np.zeros((len(test_images_ids), 512, 512, 3), dtype=np.uint8)
Y_test = np.zeros((len(test_masks_ids), 512, 512, 1), dtype=np.bool)

# Redimensión de imágenes
for n, id in tqdm(enumerate(test_images_ids), total=len(test_images_ids)):
    path = TEST_PATH + 'images/' + id
    img = imread(path)[:,:,:3]
    if img.shape[:2] != (512, 512):
        img = resize(img, (512, 512, 3), preserve_range=True, mode='constant')
    X_test[n] = img

for m, id in tqdm(enumerate(test_masks_ids), total=len(test_masks_ids)):
    path = TEST_PATH + 'masks/' + id
    mask = imread(path)
        # Asegurar que tenga un canal (grises)
    if mask.ndim == 2:
        mask = np.expand_dims(mask, axis=-1)
    elif mask.ndim == 3 and mask.shape[2] == 3:
        # Si es RGB, convertir a escala de grises
        mask = np.expand_dims(rgb2gray(mask), axis=-1)

    # Redimensionar solo si es necesario
    if mask.shape != (512, 512, 1):
        mask = resize(mask, (512, 512, 1), preserve_range=True, mode='constant', anti_aliasing=False)

    Y_test[m] = mask


# METRICAS PERSONALIZADAS

def f1_score(y_true, y_pred):
    smooth = 1e-6
    y_true = tf.cast(y_true, tf.float32)  # Convertir a float32
    y_pred = tf.cast(y_pred, tf.float32)  # Convertir a float32
    intersection = tf.reduce_sum(y_true * y_pred)
    return (2. * intersection + smooth) / (tf.reduce_sum(y_true) + tf.reduce_sum(y_pred) + smooth)

def iou_metric(y_true, y_pred):
    smooth = 1e-6
    y_true = tf.cast(y_true, tf.float32)  # Convertir a float32
    y_pred = tf.cast(y_pred, tf.float32)  # Convertir a float32
    intersection = tf.reduce_sum(y_true * y_pred)
    union = tf.reduce_sum(y_true) + tf.reduce_sum(y_pred) - intersection
    return (intersection + smooth) / (union + smooth)

def precision_metric(y_true, y_pred):
    y_true = tf.cast(y_true, tf.float32)  # Convertir a float32
    y_pred = tf.cast(y_pred, tf.float32)  # Convertir a float32
    true_positives = tf.reduce_sum(y_true * y_pred)
    predicted_positives = tf.reduce_sum(y_pred)
    return true_positives / (predicted_positives + 1e-6)

def recall_metric(y_true, y_pred):
    y_true = tf.cast(y_true, tf.float32)  # Convertir a float32
    y_pred = tf.cast(y_pred, tf.float32)  # Convertir a float32
    true_positives = tf.reduce_sum(y_true * y_pred)
    possible_positives = tf.reduce_sum(y_true)
    return true_positives / (possible_positives + 1e-6)

def Weighted_Cross_Entropy(beta):
    def convert_to_logits(y_pred):
        y_pred = tf.clip_by_value(y_pred, tf.keras.backend.epsilon(), 1 - tf.keras.backend.epsilon())
        return tf.math.log(y_pred / (1 - y_pred))

    def loss(y_true, y_pred):
        y_pred = convert_to_logits(y_pred)
        loss = tf.nn.weighted_cross_entropy_with_logits(logits=y_pred, labels=y_true, pos_weight=beta)
        return tf.reduce_mean(loss)

    return loss

def Binary_Focal_Loss(gamma=2.0, alpha=0.25):
    def loss(y_true, y_pred):
        # Evitar log(0) asegurando que los valores estén dentro de (0,1)
        y_pred = tf.clip_by_value(y_pred, tf.keras.backend.epsilon(), 1. - tf.keras.backend.epsilon())

        # Focal Loss
        cross_entropy = - (y_true * tf.math.log(y_pred) + (1 - y_true) * tf.math.log(1 - y_pred))
        focal_loss = alpha * tf.pow(1 - y_pred, gamma) * y_true * tf.math.log(y_pred) + \
                     (1 - alpha) * tf.pow(y_pred, gamma) * (1 - y_true) * tf.math.log(1 - y_pred)

        return -tf.reduce_mean(focal_loss)

    return loss



import pickle
import os

class MetricsLogger(tf.keras.callbacks.Callback):
    def __init__(self, save_path='metricas_modelo1.pkl'):
        super().__init__()
        self.save_path = save_path
        self.history = {
            'loss': [],
            'val_loss': [],
            'accuracy': [],
            'val_accuracy': [],
            'f1_score': [],
            'val_f1_score': [],
            'precision': [],
            'val_precision': [],
            'recall': [],
            'val_recall': [],
            'iou': [],
            'val_iou': []
        }

    def on_epoch_end(self, epoch, logs=None):
        # Guardar métricas de entrenamiento
        self.history['loss'].append(logs.get('loss'))
        self.history['accuracy'].append(logs.get('accuracy'))
        self.history['f1_score'].append(logs.get('f1_score'))
        self.history['precision'].append(logs.get('precision_metric'))
        self.history['recall'].append(logs.get('recall_metric'))
        self.history['iou'].append(logs.get('iou_metric'))

        # Guardar métricas de validación
        self.history['val_loss'].append(logs.get('val_loss'))
        self.history['val_accuracy'].append(logs.get('val_accuracy'))
        self.history['val_f1_score'].append(logs.get('val_f1_score'))
        self.history['val_precision'].append(logs.get('val_precision_metric'))
        self.history['val_recall'].append(logs.get('val_recall_metric'))
        self.history['val_iou'].append(logs.get('val_iou_metric'))

        # Mostrar resumen por consola
        print(f"\nEpoch {epoch+1}: "
              f"Loss={logs.get('loss'):.4f}, Val Loss={logs.get('val_loss'):.4f}, "
              f"Train F1={logs.get('f1_score'):.4f}, Val F1={logs.get('val_f1_score'):.4f}")

        # Guardar en archivo
        with open(self.save_path, 'wb') as f:
            pickle.dump(self.history, f)




def mostrar_predicciones(model, listas_imgs, img_size=512):
    filas = len(listas_imgs) * 2  # 2 filas por cada lista (imagen y predicción)
    columnas = len(listas_imgs[0])  # número de imágenes por lista

    fig, axes = plt.subplots(filas, columnas, figsize=(4 * columnas, 4 * filas))

    for fila in range(filas):
        lista_idx = fila // 2
        es_prediccion = (fila % 2 == 1)

        for col in range(columnas):
            path_img = listas_imgs[lista_idx][col]

            # Leer imagen
            img = cv2.imread(path_img)
            img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
            img_resized = cv2.resize(img_rgb, (img_size, img_size))

            if not es_prediccion:
                # Mostrar imagen original
                axes[fila, col].imshow(img_resized)
                axes[fila, col].set_title(f'Imagen {col+1} - Grupo {lista_idx+1}')
            else:
                # Mostrar predicción del modelo
                input_img = np.expand_dims(img_resized, axis=0)
                pred = model.predict(input_img)[0]
                pred = np.squeeze(pred)

                # Mostrar la predicción
                axes[fila, col].imshow(pred, cmap='gray')
                axes[fila, col].set_title(f'Predicción {col+1} - Grupo {lista_idx+1}')

            axes[fila, col].axis('off')

    plt.tight_layout()
    plt.show()






# EVOLUCIÓN DE PERDIDA ACCUARICY  F1  IOU PRECISION RECALL

import matplotlib.pyplot as plt
plt.style.use('seaborn-v0_8-colorblind')  # Estilo con fondo gris claro y colores elegantes

import matplotlib.pyplot as plt
plt.style.use('seaborn-v0_8-colorblind')  # Estilo con fondo gris claro y colores elegantes

def plot_training(history):
    epochs = range(1, len(history.history['loss']) + 1)

    # Crear figura con 3 filas y 2 columnas
    fig, axs = plt.subplots(3, 2, figsize=(14, 12))
    fig.suptitle('Evolución de métricas durante el entrenamiento', fontsize=16)

    # Buscar la época con mejor F1 Score en validación
    if 'val_f1_score' in history.history:
        best_epoch = max(range(len(history.history['val_f1_score'])),
                         key=lambda i: history.history['val_f1_score'][i])
        best_metrics = {
            'Época': best_epoch + 1,
            'F1 Score': history.history['val_f1_score'][best_epoch],
            'Precisión': history.history.get('val_precision_metric', [None])[best_epoch],
            'Recall': history.history.get('val_recall_metric', [None])[best_epoch],
            'IoU': history.history.get('val_iou_metric', [None])[best_epoch],
            'Pérdida': history.history['val_loss'][best_epoch],
            'Accuracy': history.history.get('val_accuracy', [None])[best_epoch],
        }

        texto_metricas = '\n'.join([f'{k}: {v:.4f}' for k, v in best_metrics.items() if v is not None])
    else:
        texto_metricas = 'No se encontró F1 Score para identificar la mejor época.'

    # Subgráficas
    axs[0, 0].plot(epochs, history.history['loss'], label='Entrenamiento')
    axs[0, 0].plot(epochs, history.history['val_loss'], label='Validación')
    axs[0, 0].set_title('Pérdida')
    axs[0, 0].set_xlabel('Épocas')
    axs[0, 0].set_ylabel('Pérdida')
    axs[0, 0].legend()
    axs[0, 0].grid(True, linestyle='--', alpha=0.5)

    if 'accuracy' in history.history:
        axs[0, 1].plot(epochs, history.history['accuracy'], label='Entrenamiento')
        axs[0, 1].plot(epochs, history.history['val_accuracy'], label='Validación')
        axs[0, 1].set_title('Precisión')
        axs[0, 1].set_xlabel('Épocas')
        axs[0, 1].set_ylabel('Precisión')
        axs[0, 1].legend()
        axs[0, 1].grid(True, linestyle='--', alpha=0.5)
    else:
        axs[0, 1].axis('off')

    if 'f1_score' in history.history:
        axs[1, 0].plot(epochs, history.history['f1_score'], label='Entrenamiento')
        axs[1, 0].plot(epochs, history.history['val_f1_score'], label='Validación')
        axs[1, 0].set_title('F1 Score')
        axs[1, 0].set_xlabel('Épocas')
        axs[1, 0].set_ylabel('F1 Score')
        axs[1, 0].legend()
        axs[1, 0].grid(True, linestyle='--', alpha=0.5)
    else:
        axs[1, 0].axis('off')

    if 'iou_metric' in history.history:
        axs[1, 1].plot(epochs, history.history['iou_metric'], label='Entrenamiento')
        axs[1, 1].plot(epochs, history.history['val_iou_metric'], label='Validación')
        axs[1, 1].set_title('IoU (Intersection over Union)')
        axs[1, 1].set_xlabel('Épocas')
        axs[1, 1].set_ylabel('IoU')
        axs[1, 1].legend()
        axs[1, 1].grid(True, linestyle='--', alpha=0.5)
    else:
        axs[1, 1].axis('off')

    if 'precision_metric' in history.history:
        axs[2, 0].plot(epochs, history.history['precision_metric'], label='Entrenamiento')
        axs[2, 0].plot(epochs, history.history['val_precision_metric'], label='Validación')
        axs[2, 0].set_title('Precision')
        axs[2, 0].set_xlabel('Épocas')
        axs[2, 0].set_ylabel('Precisión')
        axs[2, 0].legend()
        axs[2, 0].grid(True, linestyle='--', alpha=0.5)
    else:
        axs[2, 0].axis('off')

    if 'recall_metric' in history.history:
        axs[2, 1].plot(epochs, history.history['recall_metric'], label='Entrenamiento')
        axs[2, 1].plot(epochs, history.history['val_recall_metric'], label='Validación')
        axs[2, 1].set_title('Recall')
        axs[2, 1].set_xlabel('Épocas')
        axs[2, 1].set_ylabel('Recall')
        axs[2, 1].legend()
        axs[2, 1].grid(True, linestyle='--', alpha=0.5)
    else:
        axs[2, 1].axis('off')

    # Añadir texto con mejores métricas en el lado derecho
    fig.text(1.02, 0.5, f'Mejores métricas\n(época con mayor F1):\n\n{texto_metricas}',
             fontsize=10, bbox=dict(facecolor='white', alpha=0.8, edgecolor='gray'),
             va='center', ha='left')

    # Ajuste para dejar espacio a la derecha
    plt.tight_layout(rect=[0, 0.05, 0.95, 0.96])
    plt.savefig("metricas_entrenamiento_completo.png", dpi=300, bbox_inches='tight')
    plt.show()


# CURVA PER ROC MATRIZ MC BARRAS DISTRIBUCION

import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import (
    precision_recall_curve, roc_curve, auc,
    confusion_matrix, ConfusionMatrixDisplay
)

def mostrar_métricas(model, X_val, Y_val):
    # Predicciones
    y_pred_probs = model.predict(X_val).ravel()
    y_pred_class = (y_pred_probs > 0.5).astype(int)
    y_true_flat = Y_val.ravel()

    # Crear una figura con 2 filas y 2 columnas
    fig, axs = plt.subplots(2, 2, figsize=(12, 10))

    # 1. Precision vs Recall
    precision, recall, _ = precision_recall_curve(y_true_flat, y_pred_probs)
    pr_auc = auc(recall, precision)
    axs[0, 0].plot(recall, precision, label=f'AUC = {pr_auc:.2f}')
    axs[0, 0].set_xlabel('Recall')
    axs[0, 0].set_ylabel('Precision')
    axs[0, 0].set_title('Precision vs Recall')
    axs[0, 0].legend(loc='lower left')
    axs[0, 0].grid(True)

    # 2. Curva ROC
    fpr, tpr, _ = roc_curve(y_true_flat, y_pred_probs)
    roc_auc = auc(fpr, tpr)
    axs[0, 1].plot(fpr, tpr, label=f'AUC = {roc_auc:.2f}')
    axs[0, 1].plot([0, 1], [0, 1], 'k--')
    axs[0, 1].set_xlabel('False Positive Rate')
    axs[0, 1].set_ylabel('True Positive Rate')
    axs[0, 1].set_title('Curva ROC')
    axs[0, 1].legend(loc='lower right')
    axs[0, 1].grid(True)

    # 3. Distribución del error absoluto
    errors = np.abs(Y_val - y_pred_probs.reshape(Y_val.shape))
    axs[1, 0].hist(errors.ravel(), bins=50)
    axs[1, 0].set_title('Distribución del error absoluto')
    axs[1, 0].set_xlabel('Error')
    axs[1, 0].set_ylabel('Frecuencia')
    axs[1, 0].grid(True)

    # 4. Matriz de Confusión en porcentaje
    conf_mat = confusion_matrix(y_true_flat, y_pred_class, normalize='true')
    conf_mat_percent = np.round(conf_mat * 100, 2)
    disp = ConfusionMatrixDisplay(confusion_matrix=conf_mat_percent)
    disp.plot(cmap='Blues', values_format='.2f', ax=axs[1, 1])
    axs[1, 1].set_title("Matriz de Confusión (%)")

    # Ajustar el layout y mostrar la figura
    plt.tight_layout()
    plt.show()




from tensorflow.keras.applications import MobileNet
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Lambda, Conv2D, Conv2DTranspose, Concatenate, BatchNormalization, Activation, Dropout

import tensorflow as tf
from tensorflow.keras.optimizers import Adam

import tensorflow as tf
import os
import random
import numpy as np


# ------------------------------------------------------------------------------
# SEMILLAS PARA REPRODUCIBILIDAD
os.environ['PYTHONHASHSEED'] = '42'
os.environ['TF_DETERMINISTIC_OPS'] = '1'

random.seed(42)
np.random.seed(42)
tf.random.set_seed(42)

def conv_block(inputs, num_filters):
    x = Conv2D(num_filters, 3,kernel_initializer="he_normal", padding="same")(inputs)
    x = BatchNormalization()(x)
    x = Activation("relu")(x)
    x = Dropout(0.1)(x)

    x = Conv2D(num_filters, 3,kernel_initializer="he_normal", padding="same")(x)
    x = BatchNormalization()(x)
    x = Activation("relu")(x)

    return x

def decoder_block(inputs, skip, num_filters):
    x = Conv2DTranspose(num_filters, (2, 2), strides=2, padding="same")(inputs)
    x = Concatenate()([x, skip])
    x = conv_block(x, num_filters)

    return x

# Input
inputs = Input((512,512,3))
x = Lambda(lambda x: x / 255)(inputs)

# Pre-trained MobileNetV2
encoder = MobileNet(include_top=False, weights="imagenet",input_tensor=x, alpha=1)
encoder.summary()

# Encoder
s1 = encoder.get_layer("lambda").output
s2 = encoder.get_layer("conv_pw_1_relu").output
s3 = encoder.get_layer("conv_pw_3_relu").output
s4 = encoder.get_layer("conv_pw_5_relu").output

# Bridge
b1 = encoder.get_layer("conv_pw_11_relu").output

# Decoder
d1 = decoder_block(b1, s4, 128)                         ## 512
d2 = decoder_block(d1, s3, 64)                          ## 256
d3 = decoder_block(d2, s2, 32)                          ## 128
d4 = decoder_block(d3, s1, 16)                          ## 64

# Output
outputs = Conv2D(1, 1, padding="same", activation="sigmoid")(d4)

model = Model(inputs=[inputs], outputs=[outputs])




model.compile(optimizer = Adam(learning_rate=0.001),
              loss=Weighted_Cross_Entropy(10),
              metrics=['accuracy', f1_score, iou_metric, precision_metric, recall_metric])

model.summary()


# CALLBACKS

checkpointer = tf.keras.callbacks.ModelCheckpoint(
    'modelo2_b2.h5',
    save_best_only=True,
    monitor='val_f1_score',
    mode='max',
    verbose=1,
    save_weights_only=False
)

early_stopping = tf.keras.callbacks.EarlyStopping(
    monitor='val_f1_score',
    mode='max',
    patience=50,
    restore_best_weights=True
)

logger = MetricsLogger('modelo2_b2.pkl')

callbacks = [
    checkpointer,
    early_stopping,
    tf.keras.callbacks.TensorBoard(log_dir='logs'),
    logger
]

# ENTRENAMIENTO
results4 = model.fit(
    X_train3, Y_train3,
    validation_data=(X_test, Y_test),
    batch_size=4,
    epochs=200,
    shuffle=True,
    callbacks=callbacks
)



# Cargar modelo y visualizar
from tensorflow.keras.models import load_model
# modelo4 = load_model('/content/modelo4.h5')
modelo4 = load_model('/content/modelo2_b2.h5', custom_objects={'loss': Weighted_Cross_Entropy(10.0)})










